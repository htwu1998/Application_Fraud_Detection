{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "polyphonic-producer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T19:23:13.897219Z",
     "start_time": "2021-03-21T19:23:12.775169Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats as sps\n",
    "import datetime as dt\n",
    "import calendar\n",
    "\n",
    "# model data (train+test) csv file path\n",
    "data = pd.read_csv('train_test.csv')\n",
    "oot = pd.read_csv('oot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "yellow-default",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T18:27:31.723662Z",
     "start_time": "2021-03-21T18:27:31.718686Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(794996, 31)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "likely-looking",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T18:27:32.604604Z",
     "start_time": "2021-03-21T18:27:32.602598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166493, 31)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-return",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-davis",
   "metadata": {},
   "source": [
    "## Define PyTorch NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "foreign-recycling",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T20:10:53.723417Z",
     "start_time": "2021-03-21T20:10:53.687399Z"
    }
   },
   "outputs": [],
   "source": [
    "# load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def PyTorch_nn(data, oot, n_kfold=10, lr=0.0001, epochs = 50, batch_size=64, layer_1=64, layer_2=64):\n",
    "    \n",
    "    #Adding (multi) GPU support with DataParallel\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    class trainData(Dataset):\n",
    "    \n",
    "        def __init__(self, X_data, y_data):\n",
    "            self.X_data = X_data\n",
    "            self.y_data = y_data\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            return self.X_data[index], self.y_data[index]\n",
    "\n",
    "        def __len__ (self):\n",
    "            return len(self.X_data)\n",
    "\n",
    "\n",
    "    ## test data    \n",
    "    class testData(Dataset):\n",
    "\n",
    "        def __init__(self, X_data):\n",
    "            self.X_data = X_data\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            return self.X_data[index]\n",
    "\n",
    "        def __len__ (self):\n",
    "            return len(self.X_data)\n",
    "\n",
    "    class binaryClassification(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(binaryClassification, self).__init__()\n",
    "            # Number of input features is 30.\n",
    "            self.layer_1 = nn.Linear(30, layer_1) \n",
    "            self.layer_2 = nn.Linear(layer_1, layer_2)\n",
    "            self.layer_out = nn.Linear(layer_2, 1) \n",
    "\n",
    "            self.relu = nn.ReLU()\n",
    "            self.dropout = nn.Dropout(p=0.1)\n",
    "            self.batchnorm1 = nn.BatchNorm1d(layer_1)\n",
    "            self.batchnorm2 = nn.BatchNorm1d(layer_2)\n",
    "\n",
    "        def forward(self, inputs):\n",
    "            x = self.relu(self.layer_1(inputs))\n",
    "            x = self.batchnorm1(x)\n",
    "            x = self.relu(self.layer_2(x))\n",
    "            x = self.batchnorm2(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.layer_out(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    # initialize optimizer and decide loss function\n",
    "    model = binaryClassification()\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss() # loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    def binary_acc(y_pred, y_test):\n",
    "        y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "        correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "        acc = correct_results_sum/y_test.shape[0]\n",
    "        acc = torch.round(acc * 100)\n",
    "\n",
    "        return acc\n",
    "    \n",
    "    ########################################### MODELLING STARTS HERE ##########################################\n",
    "    \n",
    "    X = data.iloc[:,:-1]\n",
    "    y = data.iloc[:,-1]\n",
    "    oot_X = oot.iloc[:,:-1]\n",
    "    oot_y = oot.iloc[:,-1]\n",
    "\n",
    "    kf = KFold(n_splits=n_kfold)\n",
    "    kf.get_n_splits(X)\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "\n",
    "        X_train, X_test = X.iloc[train_index,], X.iloc[test_index,]\n",
    "        y_train, y_test = y.iloc[train_index,], y.iloc[test_index,]\n",
    "\n",
    "        # standardize train_test_oot data\n",
    "        scaler = StandardScaler() \n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        oot_X = scaler.transform(oot_X)\n",
    "\n",
    "        # oversampling\n",
    "        os = SMOTE()\n",
    "        columns = X.columns\n",
    "        os_data_X,os_data_y=os.fit_resample(X_train, y_train)\n",
    "        os_data_X = pd.DataFrame(data=os_data_X,columns=columns)\n",
    "        os_data_y= pd.DataFrame(data=os_data_y,columns=['fraud_label'])\n",
    "        os_data_y = os_data_y['fraud_label']\n",
    "\n",
    "        # set up train_test data and load dataset into pytorch\n",
    "        train_data = trainData(torch.FloatTensor(os_data_X.to_numpy()), \\\n",
    "                               torch.FloatTensor(os_data_y.to_numpy()))\n",
    "\n",
    "        #train_data = trainData(torch.FloatTensor(X.to_numpy()), \\\n",
    "                          #     torch.FloatTensor(y.to_numpy()))\n",
    "\n",
    "        test_data = testData(torch.FloatTensor(X_test))\n",
    "\n",
    "        train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "        #test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "        #oot_loader = DataLoader(dataset=oot_X, batch_size=1)\n",
    "\n",
    "        model.train()\n",
    "        for e in range(1, epochs+1):\n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                y_pred = model(X_batch)\n",
    "\n",
    "                loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "                acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "\n",
    "\n",
    "            #print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')\n",
    "            print(f'finish {e+0:03}', end=' ')\n",
    "            ################################## MODELLING WITH TRAIN DATA ENDS HERE ##############################\n",
    "    \n",
    "    ############## EVALUATE TEST DATA STARTS HERE ##################\n",
    "    \n",
    "    y_test_pred_list = []\n",
    "    y_test_pred_prob_list = []\n",
    "    model.eval()\n",
    "\n",
    "    scaler = StandardScaler() \n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    test_data = testData(torch.FloatTensor(X_test))\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_test_pred = model(X_batch)\n",
    "            y_test_pred = torch.sigmoid(y_test_pred)\n",
    "            y_test_pred_prob_list.append(y_test_pred)\n",
    "            y_test_pred_tag = torch.round(y_test_pred)\n",
    "            y_test_pred_list.append(y_test_pred_tag.cpu().numpy())\n",
    "\n",
    "\n",
    "    y_test_pred_list = [a.squeeze().tolist() for a in y_test_pred_list]\n",
    "    y_test_pred_prob_list = [a.squeeze().tolist() for a in y_test_pred_prob_list]\n",
    "\n",
    "    result = pd.DataFrame(y_test_pred_prob_list,columns=['prob_1'])\n",
    "    result.loc[:,'fraud_label'] = y_test.values.reshape(-1,)\n",
    "    result.loc[:,'pred_class'] = pd.Series(y_test_pred_list).values.reshape(-1,)\n",
    "    temp = result.sort_values('prob_1', ascending=False)\n",
    "    pop = int(round(temp.shape[0]*0.03))\n",
    "    temp1 = temp.head(pop)\n",
    "    fdr = temp1.fraud_label.sum() / y_test.sum()\n",
    "    \n",
    "    print(f'\\nTEST DATA - FDR @3%  {round(fdr,4)}')\n",
    "    \n",
    "    \n",
    "    ################### TEST DATA EVALUATION ENDS HERE #######################\n",
    "    ##########################################################################\n",
    "    ################### EVALUATE OOT DATA STARTS HERE ########################\n",
    "    \n",
    "    y_oot_pred_list = []\n",
    "    y_oot_pred_prob_list = []\n",
    "    model.eval()\n",
    "\n",
    "    oot_X = oot.iloc[:,:-1]\n",
    "    oot_y = oot.iloc[:,-1]\n",
    "    scaler = StandardScaler() \n",
    "    oot_X = scaler.fit_transform(oot_X)\n",
    "    oot_X = testData(torch.FloatTensor(oot_X))\n",
    "    oot_loader = DataLoader(dataset=oot_X, batch_size=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch in oot_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_oot_pred = model(X_batch)\n",
    "            y_oot_pred = torch.sigmoid(y_oot_pred)\n",
    "            y_oot_pred_prob_list.append(y_oot_pred)\n",
    "            y_oot_pred_tag = torch.round(y_oot_pred)\n",
    "            y_oot_pred_list.append(y_oot_pred_tag.cpu().numpy())\n",
    "\n",
    "\n",
    "    y_oot_pred_list = [a.squeeze().tolist() for a in y_oot_pred_list]\n",
    "    y_oot_pred_prob_list = [a.squeeze().tolist() for a in y_oot_pred_prob_list]\n",
    "\n",
    "    result1 = pd.DataFrame(y_oot_pred_prob_list,columns=['prob_1'])\n",
    "    result1.loc[:,'fraud_label'] = oot_y.values.reshape(-1,1)\n",
    "    result1.loc[:,'pred_class'] = pd.Series(y_oot_pred_list).values.reshape(-1,1)\n",
    "    temp11 = result1.sort_values('prob_1', ascending=False)\n",
    "    pop = int(round(temp11.shape[0]*0.03))\n",
    "    temp12 = temp11.head(pop)\n",
    "    fdr_oot = temp12.fraud_label.sum() / oot_y.sum()\n",
    "    print(f'OOT DATA - FDR @3%  {round(fdr_oot,4)}')\n",
    "    return fdr, fdr_oot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-brisbane",
   "metadata": {},
   "source": [
    "## Define Model w/o oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "developed-timber",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T21:45:44.749048Z",
     "start_time": "2021-03-21T21:45:44.740979Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "794991    0\n",
       "794992    0\n",
       "794993    0\n",
       "794994    0\n",
       "794995    0\n",
       "Name: fraud_label, Length: 794996, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data.iloc[:,-1]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "twelve-tennessee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T21:46:37.482970Z",
     "start_time": "2021-03-21T21:46:37.450163Z"
    }
   },
   "outputs": [],
   "source": [
    "# load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def PyTorch_nn(data, oot, n_kfold=10, lr=0.0001, epochs = 50, batch_size=64, layer_1=64, layer_2=64):\n",
    "    \n",
    "    #Adding (multi) GPU support with DataParallel\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    class trainData(Dataset):\n",
    "    \n",
    "        def __init__(self, X_data, y_data):\n",
    "            self.X_data = X_data\n",
    "            self.y_data = y_data\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            return self.X_data[index], self.y_data[index]\n",
    "\n",
    "        def __len__ (self):\n",
    "            return len(self.X_data)\n",
    "\n",
    "\n",
    "    ## test data    \n",
    "    class testData(Dataset):\n",
    "\n",
    "        def __init__(self, X_data):\n",
    "            self.X_data = X_data\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            return self.X_data[index]\n",
    "\n",
    "        def __len__ (self):\n",
    "            return len(self.X_data)\n",
    "\n",
    "    class binaryClassification(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(binaryClassification, self).__init__()\n",
    "            # Number of input features is 30.\n",
    "            self.layer_1 = nn.Linear(30, layer_1) \n",
    "            self.layer_2 = nn.Linear(layer_1, layer_2)\n",
    "            self.layer_out = nn.Linear(layer_2, 1) \n",
    "\n",
    "            self.relu = nn.ReLU()\n",
    "            self.dropout = nn.Dropout(p=0.1)\n",
    "            self.batchnorm1 = nn.BatchNorm1d(layer_1)\n",
    "            self.batchnorm2 = nn.BatchNorm1d(layer_2)\n",
    "\n",
    "        def forward(self, inputs):\n",
    "            x = self.relu(self.layer_1(inputs))\n",
    "            x = self.batchnorm1(x)\n",
    "            x = self.relu(self.layer_2(x))\n",
    "            x = self.batchnorm2(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.layer_out(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    # initialize optimizer and decide loss function\n",
    "    model = binaryClassification()\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss() # loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    def binary_acc(y_pred, y_test):\n",
    "        y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "        correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "        acc = correct_results_sum/y_test.shape[0]\n",
    "        acc = torch.round(acc * 100)\n",
    "\n",
    "        return acc\n",
    "    \n",
    "    ########################################### MODELLING STARTS HERE ##########################################\n",
    "    \n",
    "    X = data.iloc[:,:-1]\n",
    "    y = data.iloc[:,-1]\n",
    "    oot_X = oot.iloc[:,:-1]\n",
    "    oot_y = oot.iloc[:,-1]\n",
    "\n",
    "    kf = KFold(n_splits=n_kfold)\n",
    "    kf.get_n_splits(X)\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "\n",
    "        X_train, X_test = X.iloc[train_index,], X.iloc[test_index,]\n",
    "        y_train, y_test = y.iloc[train_index,], y.iloc[test_index,]\n",
    "\n",
    "        # standardize train_test_oot data\n",
    "        scaler = StandardScaler() \n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        oot_X = scaler.transform(oot_X)\n",
    "\n",
    "        # oversampling\n",
    "        #os = SMOTE()\n",
    "        #columns = X.columns\n",
    "        #os_data_X,os_data_y=os.fit_resample(X_train, y_train)\n",
    "        #os_data_X = pd.DataFrame(data=os_data_X,columns=columns)\n",
    "        #os_data_y= pd.DataFrame(data=os_data_y,columns=['fraud_label'])\n",
    "        #os_data_y = os_data_y['fraud_label']\n",
    "\n",
    "        # set up train_test data and load dataset into pytorch\n",
    "        train_data = trainData(torch.FloatTensor(X_train), \\\n",
    "                               torch.FloatTensor(y_train.to_numpy()))\n",
    "\n",
    "        #train_data = trainData(torch.FloatTensor(X.to_numpy()), \\\n",
    "                          #     torch.FloatTensor(y.to_numpy()))\n",
    "\n",
    "        test_data = testData(torch.FloatTensor(X_test))\n",
    "\n",
    "        train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "        #test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "        #oot_loader = DataLoader(dataset=oot_X, batch_size=1)\n",
    "\n",
    "        model.train()\n",
    "        for e in range(1, epochs+1):\n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                y_pred = model(X_batch)\n",
    "\n",
    "                loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "                acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "\n",
    "\n",
    "            #print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')\n",
    "            print(f'finish {e+0:03}', end=' ')\n",
    "            ################################## MODELLING WITH TRAIN DATA ENDS HERE ##############################\n",
    "    \n",
    "    ############## EVALUATE TEST DATA STARTS HERE ##################\n",
    "    \n",
    "    y_test_pred_list = []\n",
    "    y_test_pred_prob_list = []\n",
    "    model.eval()\n",
    "\n",
    "    scaler = StandardScaler() \n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    test_data = testData(torch.FloatTensor(X_test))\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_test_pred = model(X_batch)\n",
    "            y_test_pred = torch.sigmoid(y_test_pred)\n",
    "            y_test_pred_prob_list.append(y_test_pred)\n",
    "            y_test_pred_tag = torch.round(y_test_pred)\n",
    "            y_test_pred_list.append(y_test_pred_tag.cpu().numpy())\n",
    "\n",
    "\n",
    "    y_test_pred_list = [a.squeeze().tolist() for a in y_test_pred_list]\n",
    "    y_test_pred_prob_list = [a.squeeze().tolist() for a in y_test_pred_prob_list]\n",
    "\n",
    "    result = pd.DataFrame(y_test_pred_prob_list,columns=['prob_1'])\n",
    "    result.loc[:,'fraud_label'] = y_test.values.reshape(-1,)\n",
    "    result.loc[:,'pred_class'] = pd.Series(y_test_pred_list).values.reshape(-1,)\n",
    "    temp = result.sort_values('prob_1', ascending=False)\n",
    "    pop = int(round(temp.shape[0]*0.03))\n",
    "    temp1 = temp.head(pop)\n",
    "    fdr = temp1.fraud_label.sum() / y_test.sum()\n",
    "    \n",
    "    print(f'\\nTEST DATA - FDR @3%  {round(fdr,4)}')\n",
    "    \n",
    "    \n",
    "    ################### TEST DATA EVALUATION ENDS HERE #######################\n",
    "    ##########################################################################\n",
    "    ################### EVALUATE OOT DATA STARTS HERE ########################\n",
    "    \n",
    "    y_oot_pred_list = []\n",
    "    y_oot_pred_prob_list = []\n",
    "    model.eval()\n",
    "\n",
    "    oot_X = oot.iloc[:,:-1]\n",
    "    oot_y = oot.iloc[:,-1]\n",
    "    scaler = StandardScaler() \n",
    "    oot_X = scaler.fit_transform(oot_X)\n",
    "    oot_X = testData(torch.FloatTensor(oot_X))\n",
    "    oot_loader = DataLoader(dataset=oot_X, batch_size=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch in oot_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_oot_pred = model(X_batch)\n",
    "            y_oot_pred = torch.sigmoid(y_oot_pred)\n",
    "            y_oot_pred_prob_list.append(y_oot_pred)\n",
    "            y_oot_pred_tag = torch.round(y_oot_pred)\n",
    "            y_oot_pred_list.append(y_oot_pred_tag.cpu().numpy())\n",
    "\n",
    "\n",
    "    y_oot_pred_list = [a.squeeze().tolist() for a in y_oot_pred_list]\n",
    "    y_oot_pred_prob_list = [a.squeeze().tolist() for a in y_oot_pred_prob_list]\n",
    "\n",
    "    result1 = pd.DataFrame(y_oot_pred_prob_list,columns=['prob_1'])\n",
    "    result1.loc[:,'fraud_label'] = oot_y.values.reshape(-1,1)\n",
    "    result1.loc[:,'pred_class'] = pd.Series(y_oot_pred_list).values.reshape(-1,1)\n",
    "    temp11 = result1.sort_values('prob_1', ascending=False)\n",
    "    pop = int(round(temp11.shape[0]*0.03))\n",
    "    temp12 = temp11.head(pop)\n",
    "    fdr_oot = temp12.fraud_label.sum() / oot_y.sum()\n",
    "    print(f'OOT DATA - FDR @3%  {round(fdr_oot,4)}')\n",
    "    \n",
    "    return fdr, fdr_oot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "charged-falls",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T22:55:16.077376Z",
     "start_time": "2021-03-21T22:19:49.311004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 001 finish 002 finish 003 finish 004 finish 005 finish 006 finish 007 finish 008 finish 009 finish 010 finish 001 finish 002 finish 003 finish 004 finish 005 finish 006 finish 007 finish 008 finish 009 finish 010 finish 001 finish 002 finish 003 finish 004 finish 005 finish 006 finish 007 finish 008 finish 009 finish 010 finish 001 finish 002 finish 003 finish 004 finish 005 finish 006 finish 007 finish 008 finish 009 finish 010 finish 001 finish 002 finish 003 finish 004 finish 005 finish 006 finish 007 finish 008 finish 009 finish 010 finish 001 finish 002 finish 003 finish 004 finish 005 finish 006 finish 007 finish 008 finish 009 finish 010 finish 001 finish 002 finish 003 finish 004 finish 005 finish 006 finish 007 finish 008 finish 009 finish 010 finish 001 finish 002 finish 003 finish 004 finish 005 finish 006 finish 007 finish 008 finish 009 finish 010 finish 001 finish 002 finish 003 finish 004 finish 005 finish 006 finish 007 finish 008 finish 009 finish 010 finish 001 finish 002 finish 003 finish 004 finish 005 finish 006 finish 007 finish 008 finish 009 finish 010 \n",
      "TEST DATA - FDR @3%  0.5852\n",
      "OOT DATA - FDR @3%  0.552\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5852225020990764, 0.5519698239731768)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trial \n",
    "PyTorch_nn(data, oot, n_kfold=10, lr=0.0001, epochs = 10, batch_size=64, layer_1=64, layer_2=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "clean-vertex",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T07:50:24.101603Z",
     "start_time": "2021-03-22T07:15:14.611045Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   lr   | epochs | batch_size | layer_1 | layer_2 |\n",
      "| 0.0001 |  015   |     064    |   128   |   128   |\n",
      "finish 001 finish 002 finish 003 finish 004 finish 005 finish 006 finish 007 finish 008 finish 009 finish 010 finish 011 finish 012 finish 013 finish 014 finish 015 finish 001 finish 002 finish 003 finish 004 finish 005 finish 006 finish 007 finish 008 finish 009 finish 010 finish 011 finish 012 finish 013 finish 014 finish 015 finish 001 finish 002 finish 003 finish 004 finish 005 finish 006 finish 007 finish 008 finish 009 finish 010 finish 011 finish 012 finish 013 finish 014 finish 015 finish 001 finish 002 finish 003 finish 004 finish 005 finish 006 finish 007 finish 008 finish 009 finish 010 finish 011 finish 012 finish 013 finish 014 finish 015 finish 001 finish 002 finish 003 finish 004 finish 005 finish 006 finish 007 finish 008 finish 009 finish 010 finish 011 finish 012 finish 013 finish 014 finish 015 \n",
      "TEST DATA - FDR @3%  0.6019\n",
      "OOT DATA - FDR @3%  0.5541\n"
     ]
    }
   ],
   "source": [
    "# Best model\n",
    "\n",
    "# customize hyperparameters\n",
    "n_kfold = [5]\n",
    "lr = [0.0001]\n",
    "epochs = [15]\n",
    "batch_size = [64]\n",
    "layer_1 = [128] \n",
    "layer_2 = [128] \n",
    "df = pd.DataFrame(0, columns =['n_kfold','learning rate','epochs','batch_size','layer_1','layer_2','TEST fdr@3%', 'OOT fdr@3%'], index=range(1))\n",
    "j=0\n",
    "\n",
    "data = pd.read_csv('train_test.csv')\n",
    "oot = pd.read_csv('oot.csv')\n",
    "print(f'|   lr   | epochs | batch_size | layer_1 | layer_2 |')\n",
    "\n",
    "#################### for loops to experiment hyperparameters ###########################\n",
    "for n in n_kfold:\n",
    "    for l in lr:\n",
    "        for epoch in epochs:\n",
    "            for b in batch_size:\n",
    "                for l1 in layer_1:\n",
    "                    for l2 in layer_2:\n",
    "                        df.loc[j,'n_kfold'] = n\n",
    "                        df.loc[j,'learning rate']=l\n",
    "                        df.loc[j,'epochs']=epoch\n",
    "                        df.loc[j,'batch_size'] = b\n",
    "                        df.loc[j,'layer_1']=l1\n",
    "                        df.loc[j,'layer_2']=l2\n",
    "                        \n",
    "                        print(f'| {l:.4f} |  {epoch+0:03}   |     {b+0:03}    |   {l1+0:03}   |   {l2+0:03}   |')\n",
    "                        fdr, fdr_oot = PyTorch_nn(data=data, oot=oot, n_kfold=n, lr=l, epochs = epoch, batch_size=b, layer_1=l1, layer_2=l2)\n",
    "                        df.loc[j, 'TEST fdr@3%'] = fdr\n",
    "                        df.loc[j, 'OOT fdr@3%'] = fdr_oot\n",
    "                        j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aggregate-browser",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T07:50:54.030376Z",
     "start_time": "2021-03-22T07:50:54.018161Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_kfold</th>\n",
       "      <th>learning rate</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>layer_1</th>\n",
       "      <th>layer_2</th>\n",
       "      <th>TEST fdr@3%</th>\n",
       "      <th>OOT fdr@3%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>15</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>0.601934</td>\n",
       "      <td>0.554065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_kfold  learning rate  epochs  batch_size  layer_1  layer_2  TEST fdr@3%  \\\n",
       "0        5         0.0001      15          64      128      128     0.601934   \n",
       "\n",
       "   OOT fdr@3%  \n",
       "0    0.554065  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best model hyperparameter table\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "latin-fruit",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T06:39:14.865870Z",
     "start_time": "2021-03-22T06:39:14.845896Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_kfold</th>\n",
       "      <th>learning rate</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>layer_1</th>\n",
       "      <th>layer_2</th>\n",
       "      <th>TEST fdr@3%</th>\n",
       "      <th>OOT fdr@3%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>15.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.577059</td>\n",
       "      <td>0.547779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>15.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.575038</td>\n",
       "      <td>0.545683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>15.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.574533</td>\n",
       "      <td>0.546102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>15.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.581607</td>\n",
       "      <td>0.551970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_kfold  learning rate  epochs  batch_size  layer_1  layer_2  TEST fdr@3%  \\\n",
       "0      3.0         0.0001    15.0        64.0     32.0     32.0     0.577059   \n",
       "1      3.0         0.0001    15.0        64.0     32.0     64.0     0.575038   \n",
       "2      3.0         0.0001    15.0        64.0     64.0     32.0     0.574533   \n",
       "3      3.0         0.0001    15.0        64.0     64.0     64.0     0.581607   \n",
       "\n",
       "   OOT fdr@3%  \n",
       "0    0.547779  \n",
       "1    0.545683  \n",
       "2    0.546102  \n",
       "3    0.551970  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trials \n",
    "df1 = df.copy()\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-duration",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# CODES (BACKUP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "plastic-seven",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T06:44:18.815307Z",
     "start_time": "2021-03-21T06:44:18.788509Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Set up model structure\n",
    "# can adjust layer_1, layer_2,\n",
    "## train data\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.0001\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class trainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "\n",
    "## test data    \n",
    "class testData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "class binaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(binaryClassification, self).__init__()\n",
    "        # Number of input features is 30.\n",
    "        self.layer_1 = nn.Linear(30, 64) \n",
    "        self.layer_2 = nn.Linear(64, 64)\n",
    "        self.layer_out = nn.Linear(64, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def fdr_cal(x_data, y_data, model):\n",
    "    model = model\n",
    "    pop = int(round(len(x_data)*0.03))\n",
    "    result = pd.DataFrame(model.predict_proba(x_data),columns=['prob_0', 'prob_1'])\n",
    "    temp = x_data.copy()\n",
    "    temp['fraud_label'] = y_data\n",
    "    temp['prob_1']= list(result.prob_1)\n",
    "    temp0 = temp.sort_values('prob_1', ascending=False)\n",
    "    temp1 = temp0.head(pop)\n",
    "    fdr = temp1.fraud_label.sum() / y_data.sum()\n",
    "\n",
    "    return fdr\n",
    "\n",
    "# initialize optimizer and decide loss function\n",
    "model = binaryClassification()\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss() # loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "prospective-cholesterol",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T17:58:30.570568Z",
     "start_time": "2021-03-21T17:58:30.512743Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model preparation\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "y = data.iloc[:,-1]\n",
    "oot_X = oot.iloc[:,:-1]\n",
    "oot_y = oot.iloc[:,-1]\n",
    "\n",
    "kf = KFold(n_splits=n_kfold)\n",
    "kf.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "unusual-founder",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T06:44:23.650166Z",
     "start_time": "2021-03-21T06:44:23.645714Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1567020, 30)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os_data_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sitting-freight",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T09:13:48.177940Z",
     "start_time": "2021-03-21T06:44:29.063081Z"
    },
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.45789 | Acc: 77.836\n",
      "Epoch 002: | Loss: 0.44917 | Acc: 78.222\n",
      "Epoch 003: | Loss: 0.44701 | Acc: 78.300\n",
      "Epoch 004: | Loss: 0.44582 | Acc: 78.344\n",
      "Epoch 005: | Loss: 0.44501 | Acc: 78.375\n",
      "Epoch 006: | Loss: 0.44422 | Acc: 78.404\n",
      "Epoch 007: | Loss: 0.44366 | Acc: 78.436\n",
      "Epoch 008: | Loss: 0.44316 | Acc: 78.455\n",
      "Epoch 009: | Loss: 0.44275 | Acc: 78.456\n",
      "Epoch 010: | Loss: 0.44224 | Acc: 78.474\n",
      "Epoch 011: | Loss: 0.44171 | Acc: 78.501\n",
      "Epoch 012: | Loss: 0.44132 | Acc: 78.526\n",
      "Epoch 013: | Loss: 0.44121 | Acc: 78.522\n",
      "Epoch 014: | Loss: 0.44079 | Acc: 78.537\n",
      "Epoch 015: | Loss: 0.44041 | Acc: 78.555\n",
      "Epoch 016: | Loss: 0.44016 | Acc: 78.556\n",
      "Epoch 017: | Loss: 0.44006 | Acc: 78.551\n",
      "Epoch 018: | Loss: 0.43995 | Acc: 78.573\n",
      "Epoch 019: | Loss: 0.43960 | Acc: 78.568\n",
      "Epoch 020: | Loss: 0.43937 | Acc: 78.594\n",
      "Epoch 021: | Loss: 0.43928 | Acc: 78.591\n",
      "Epoch 022: | Loss: 0.43921 | Acc: 78.583\n",
      "Epoch 023: | Loss: 0.43921 | Acc: 78.608\n",
      "Epoch 024: | Loss: 0.43893 | Acc: 78.606\n",
      "Epoch 025: | Loss: 0.43902 | Acc: 78.606\n",
      "Epoch 026: | Loss: 0.43887 | Acc: 78.610\n",
      "Epoch 027: | Loss: 0.43850 | Acc: 78.619\n",
      "Epoch 028: | Loss: 0.43849 | Acc: 78.623\n",
      "Epoch 029: | Loss: 0.43828 | Acc: 78.625\n",
      "Epoch 030: | Loss: 0.43826 | Acc: 78.638\n",
      "Epoch 031: | Loss: 0.43805 | Acc: 78.637\n",
      "Epoch 032: | Loss: 0.43784 | Acc: 78.653\n",
      "Epoch 033: | Loss: 0.43766 | Acc: 78.653\n",
      "Epoch 034: | Loss: 0.43771 | Acc: 78.659\n",
      "Epoch 035: | Loss: 0.43757 | Acc: 78.672\n",
      "Epoch 036: | Loss: 0.43731 | Acc: 78.678\n",
      "Epoch 037: | Loss: 0.43729 | Acc: 78.682\n",
      "Epoch 038: | Loss: 0.43721 | Acc: 78.697\n",
      "Epoch 039: | Loss: 0.43727 | Acc: 78.682\n",
      "Epoch 040: | Loss: 0.43690 | Acc: 78.695\n",
      "Epoch 041: | Loss: 0.43683 | Acc: 78.703\n",
      "Epoch 042: | Loss: 0.43699 | Acc: 78.683\n",
      "Epoch 043: | Loss: 0.43681 | Acc: 78.706\n",
      "Epoch 044: | Loss: 0.43655 | Acc: 78.718\n",
      "Epoch 045: | Loss: 0.43644 | Acc: 78.717\n",
      "Epoch 046: | Loss: 0.43640 | Acc: 78.731\n",
      "Epoch 047: | Loss: 0.43619 | Acc: 78.730\n",
      "Epoch 048: | Loss: 0.43617 | Acc: 78.731\n",
      "Epoch 049: | Loss: 0.43605 | Acc: 78.744\n",
      "Epoch 050: | Loss: 0.43589 | Acc: 78.729\n",
      "Epoch 001: | Loss: 0.44180 | Acc: 78.311\n",
      "Epoch 002: | Loss: 0.44123 | Acc: 78.329\n",
      "Epoch 003: | Loss: 0.44085 | Acc: 78.328\n",
      "Epoch 004: | Loss: 0.44087 | Acc: 78.338\n",
      "Epoch 005: | Loss: 0.44065 | Acc: 78.349\n",
      "Epoch 006: | Loss: 0.44050 | Acc: 78.357\n",
      "Epoch 007: | Loss: 0.44057 | Acc: 78.340\n",
      "Epoch 008: | Loss: 0.44009 | Acc: 78.361\n",
      "Epoch 009: | Loss: 0.44014 | Acc: 78.365\n",
      "Epoch 010: | Loss: 0.44015 | Acc: 78.370\n",
      "Epoch 011: | Loss: 0.43994 | Acc: 78.380\n",
      "Epoch 012: | Loss: 0.43975 | Acc: 78.394\n",
      "Epoch 013: | Loss: 0.43959 | Acc: 78.380\n",
      "Epoch 014: | Loss: 0.43952 | Acc: 78.396\n",
      "Epoch 015: | Loss: 0.43956 | Acc: 78.403\n",
      "Epoch 016: | Loss: 0.43973 | Acc: 78.399\n",
      "Epoch 017: | Loss: 0.43948 | Acc: 78.405\n",
      "Epoch 018: | Loss: 0.43942 | Acc: 78.402\n",
      "Epoch 019: | Loss: 0.43923 | Acc: 78.394\n",
      "Epoch 020: | Loss: 0.43931 | Acc: 78.404\n",
      "Epoch 021: | Loss: 0.43911 | Acc: 78.415\n",
      "Epoch 022: | Loss: 0.43903 | Acc: 78.428\n",
      "Epoch 023: | Loss: 0.43903 | Acc: 78.408\n",
      "Epoch 024: | Loss: 0.43896 | Acc: 78.434\n",
      "Epoch 025: | Loss: 0.43878 | Acc: 78.426\n",
      "Epoch 026: | Loss: 0.43871 | Acc: 78.445\n",
      "Epoch 027: | Loss: 0.43872 | Acc: 78.418\n",
      "Epoch 028: | Loss: 0.43885 | Acc: 78.447\n",
      "Epoch 029: | Loss: 0.43875 | Acc: 78.444\n",
      "Epoch 030: | Loss: 0.43873 | Acc: 78.440\n",
      "Epoch 031: | Loss: 0.43854 | Acc: 78.455\n",
      "Epoch 032: | Loss: 0.43865 | Acc: 78.429\n",
      "Epoch 033: | Loss: 0.43844 | Acc: 78.449\n",
      "Epoch 034: | Loss: 0.43839 | Acc: 78.455\n",
      "Epoch 035: | Loss: 0.43828 | Acc: 78.462\n",
      "Epoch 036: | Loss: 0.43819 | Acc: 78.468\n",
      "Epoch 037: | Loss: 0.43834 | Acc: 78.456\n",
      "Epoch 038: | Loss: 0.43813 | Acc: 78.468\n",
      "Epoch 039: | Loss: 0.43823 | Acc: 78.464\n",
      "Epoch 040: | Loss: 0.43815 | Acc: 78.482\n",
      "Epoch 041: | Loss: 0.43799 | Acc: 78.481\n",
      "Epoch 042: | Loss: 0.43801 | Acc: 78.490\n",
      "Epoch 043: | Loss: 0.43767 | Acc: 78.483\n",
      "Epoch 044: | Loss: 0.43778 | Acc: 78.494\n",
      "Epoch 045: | Loss: 0.43772 | Acc: 78.508\n",
      "Epoch 046: | Loss: 0.43754 | Acc: 78.503\n",
      "Epoch 047: | Loss: 0.43758 | Acc: 78.493\n",
      "Epoch 048: | Loss: 0.43752 | Acc: 78.492\n",
      "Epoch 049: | Loss: 0.43743 | Acc: 78.503\n",
      "Epoch 050: | Loss: 0.43745 | Acc: 78.512\n",
      "Epoch 001: | Loss: 0.44305 | Acc: 78.141\n",
      "Epoch 002: | Loss: 0.44231 | Acc: 78.166\n",
      "Epoch 003: | Loss: 0.44201 | Acc: 78.195\n",
      "Epoch 004: | Loss: 0.44182 | Acc: 78.194\n",
      "Epoch 005: | Loss: 0.44189 | Acc: 78.201\n",
      "Epoch 006: | Loss: 0.44137 | Acc: 78.231\n",
      "Epoch 007: | Loss: 0.44152 | Acc: 78.231\n",
      "Epoch 008: | Loss: 0.44120 | Acc: 78.231\n",
      "Epoch 009: | Loss: 0.44131 | Acc: 78.225\n",
      "Epoch 010: | Loss: 0.44117 | Acc: 78.241\n",
      "Epoch 011: | Loss: 0.44101 | Acc: 78.253\n",
      "Epoch 012: | Loss: 0.44108 | Acc: 78.239\n",
      "Epoch 013: | Loss: 0.44095 | Acc: 78.242\n",
      "Epoch 014: | Loss: 0.44076 | Acc: 78.252\n",
      "Epoch 015: | Loss: 0.44084 | Acc: 78.260\n",
      "Epoch 016: | Loss: 0.44070 | Acc: 78.258\n",
      "Epoch 017: | Loss: 0.44056 | Acc: 78.267\n",
      "Epoch 018: | Loss: 0.44045 | Acc: 78.266\n",
      "Epoch 019: | Loss: 0.44070 | Acc: 78.262\n",
      "Epoch 020: | Loss: 0.44051 | Acc: 78.274\n",
      "Epoch 021: | Loss: 0.44044 | Acc: 78.258\n",
      "Epoch 022: | Loss: 0.44025 | Acc: 78.277\n",
      "Epoch 023: | Loss: 0.44030 | Acc: 78.281\n",
      "Epoch 024: | Loss: 0.44033 | Acc: 78.275\n",
      "Epoch 025: | Loss: 0.44040 | Acc: 78.287\n",
      "Epoch 026: | Loss: 0.44025 | Acc: 78.280\n",
      "Epoch 027: | Loss: 0.44014 | Acc: 78.284\n",
      "Epoch 028: | Loss: 0.44003 | Acc: 78.296\n",
      "Epoch 029: | Loss: 0.44010 | Acc: 78.290\n",
      "Epoch 030: | Loss: 0.43989 | Acc: 78.292\n",
      "Epoch 031: | Loss: 0.44005 | Acc: 78.289\n",
      "Epoch 032: | Loss: 0.44006 | Acc: 78.295\n",
      "Epoch 033: | Loss: 0.43994 | Acc: 78.299\n",
      "Epoch 034: | Loss: 0.43987 | Acc: 78.301\n",
      "Epoch 035: | Loss: 0.43956 | Acc: 78.313\n",
      "Epoch 036: | Loss: 0.43969 | Acc: 78.314\n",
      "Epoch 037: | Loss: 0.43978 | Acc: 78.313\n",
      "Epoch 038: | Loss: 0.43952 | Acc: 78.320\n",
      "Epoch 039: | Loss: 0.43965 | Acc: 78.301\n",
      "Epoch 040: | Loss: 0.43982 | Acc: 78.308\n",
      "Epoch 041: | Loss: 0.43934 | Acc: 78.335\n",
      "Epoch 042: | Loss: 0.43954 | Acc: 78.318\n",
      "Epoch 043: | Loss: 0.43929 | Acc: 78.342\n",
      "Epoch 044: | Loss: 0.43945 | Acc: 78.316\n",
      "Epoch 045: | Loss: 0.43935 | Acc: 78.326\n",
      "Epoch 046: | Loss: 0.43945 | Acc: 78.315\n",
      "Epoch 047: | Loss: 0.43928 | Acc: 78.334\n",
      "Epoch 048: | Loss: 0.43918 | Acc: 78.341\n",
      "Epoch 049: | Loss: 0.43932 | Acc: 78.346\n",
      "Epoch 050: | Loss: 0.43910 | Acc: 78.349\n",
      "Epoch 001: | Loss: 0.44288 | Acc: 78.124\n",
      "Epoch 002: | Loss: 0.44180 | Acc: 78.154\n",
      "Epoch 003: | Loss: 0.44180 | Acc: 78.184\n",
      "Epoch 004: | Loss: 0.44147 | Acc: 78.195\n",
      "Epoch 005: | Loss: 0.44109 | Acc: 78.207\n",
      "Epoch 006: | Loss: 0.44113 | Acc: 78.204\n",
      "Epoch 007: | Loss: 0.44105 | Acc: 78.209\n",
      "Epoch 008: | Loss: 0.44109 | Acc: 78.212\n",
      "Epoch 009: | Loss: 0.44092 | Acc: 78.221\n",
      "Epoch 010: | Loss: 0.44080 | Acc: 78.223\n",
      "Epoch 011: | Loss: 0.44078 | Acc: 78.228\n",
      "Epoch 012: | Loss: 0.44060 | Acc: 78.211\n",
      "Epoch 013: | Loss: 0.44052 | Acc: 78.221\n",
      "Epoch 014: | Loss: 0.44075 | Acc: 78.229\n",
      "Epoch 015: | Loss: 0.44062 | Acc: 78.254\n",
      "Epoch 016: | Loss: 0.44044 | Acc: 78.235\n",
      "Epoch 017: | Loss: 0.44037 | Acc: 78.242\n",
      "Epoch 018: | Loss: 0.44022 | Acc: 78.247\n",
      "Epoch 019: | Loss: 0.44022 | Acc: 78.253\n",
      "Epoch 020: | Loss: 0.44023 | Acc: 78.252\n",
      "Epoch 021: | Loss: 0.44014 | Acc: 78.243\n",
      "Epoch 022: | Loss: 0.44009 | Acc: 78.243\n",
      "Epoch 023: | Loss: 0.44001 | Acc: 78.255\n",
      "Epoch 024: | Loss: 0.43988 | Acc: 78.262\n",
      "Epoch 025: | Loss: 0.44000 | Acc: 78.255\n",
      "Epoch 026: | Loss: 0.43992 | Acc: 78.264\n",
      "Epoch 027: | Loss: 0.43980 | Acc: 78.257\n",
      "Epoch 028: | Loss: 0.44000 | Acc: 78.251\n",
      "Epoch 029: | Loss: 0.43978 | Acc: 78.255\n",
      "Epoch 030: | Loss: 0.43967 | Acc: 78.275\n",
      "Epoch 031: | Loss: 0.43971 | Acc: 78.266\n",
      "Epoch 032: | Loss: 0.43960 | Acc: 78.274\n",
      "Epoch 033: | Loss: 0.43960 | Acc: 78.280\n",
      "Epoch 034: | Loss: 0.43966 | Acc: 78.266\n",
      "Epoch 035: | Loss: 0.43967 | Acc: 78.285\n",
      "Epoch 036: | Loss: 0.43968 | Acc: 78.267\n",
      "Epoch 037: | Loss: 0.43948 | Acc: 78.289\n",
      "Epoch 038: | Loss: 0.43951 | Acc: 78.275\n",
      "Epoch 039: | Loss: 0.43936 | Acc: 78.279\n",
      "Epoch 040: | Loss: 0.43968 | Acc: 78.275\n",
      "Epoch 041: | Loss: 0.43940 | Acc: 78.284\n",
      "Epoch 042: | Loss: 0.43927 | Acc: 78.291\n",
      "Epoch 043: | Loss: 0.43934 | Acc: 78.276\n",
      "Epoch 044: | Loss: 0.43948 | Acc: 78.275\n",
      "Epoch 045: | Loss: 0.43925 | Acc: 78.285\n",
      "Epoch 046: | Loss: 0.43917 | Acc: 78.299\n",
      "Epoch 047: | Loss: 0.43910 | Acc: 78.287\n",
      "Epoch 048: | Loss: 0.43923 | Acc: 78.277\n",
      "Epoch 049: | Loss: 0.43921 | Acc: 78.302\n",
      "Epoch 050: | Loss: 0.43929 | Acc: 78.291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.44525 | Acc: 77.993\n",
      "Epoch 002: | Loss: 0.44434 | Acc: 78.019\n",
      "Epoch 003: | Loss: 0.44398 | Acc: 78.029\n",
      "Epoch 004: | Loss: 0.44376 | Acc: 78.043\n",
      "Epoch 005: | Loss: 0.44352 | Acc: 78.053\n",
      "Epoch 006: | Loss: 0.44338 | Acc: 78.063\n",
      "Epoch 007: | Loss: 0.44339 | Acc: 78.064\n",
      "Epoch 008: | Loss: 0.44317 | Acc: 78.084\n",
      "Epoch 009: | Loss: 0.44289 | Acc: 78.085\n",
      "Epoch 010: | Loss: 0.44295 | Acc: 78.094\n",
      "Epoch 011: | Loss: 0.44294 | Acc: 78.074\n",
      "Epoch 012: | Loss: 0.44354 | Acc: 78.052\n",
      "Epoch 013: | Loss: 0.44303 | Acc: 78.088\n",
      "Epoch 014: | Loss: 0.44274 | Acc: 78.097\n",
      "Epoch 015: | Loss: 0.44265 | Acc: 78.103\n",
      "Epoch 016: | Loss: 0.44276 | Acc: 78.082\n",
      "Epoch 017: | Loss: 0.44284 | Acc: 78.093\n",
      "Epoch 018: | Loss: 0.44250 | Acc: 78.101\n",
      "Epoch 019: | Loss: 0.44253 | Acc: 78.085\n",
      "Epoch 020: | Loss: 0.44260 | Acc: 78.090\n",
      "Epoch 021: | Loss: 0.44256 | Acc: 78.104\n",
      "Epoch 022: | Loss: 0.44253 | Acc: 78.101\n",
      "Epoch 023: | Loss: 0.44221 | Acc: 78.126\n",
      "Epoch 024: | Loss: 0.44232 | Acc: 78.097\n",
      "Epoch 025: | Loss: 0.44232 | Acc: 78.114\n",
      "Epoch 026: | Loss: 0.44245 | Acc: 78.102\n",
      "Epoch 027: | Loss: 0.44218 | Acc: 78.124\n",
      "Epoch 028: | Loss: 0.44229 | Acc: 78.116\n",
      "Epoch 029: | Loss: 0.44243 | Acc: 78.106\n",
      "Epoch 030: | Loss: 0.44205 | Acc: 78.121\n",
      "Epoch 031: | Loss: 0.44211 | Acc: 78.124\n",
      "Epoch 032: | Loss: 0.44205 | Acc: 78.133\n",
      "Epoch 033: | Loss: 0.44189 | Acc: 78.133\n",
      "Epoch 034: | Loss: 0.44218 | Acc: 78.118\n",
      "Epoch 035: | Loss: 0.44199 | Acc: 78.118\n",
      "Epoch 036: | Loss: 0.44183 | Acc: 78.129\n",
      "Epoch 037: | Loss: 0.44175 | Acc: 78.140\n",
      "Epoch 038: | Loss: 0.44175 | Acc: 78.147\n",
      "Epoch 039: | Loss: 0.44203 | Acc: 78.123\n",
      "Epoch 040: | Loss: 0.44189 | Acc: 78.128\n",
      "Epoch 041: | Loss: 0.44186 | Acc: 78.126\n",
      "Epoch 042: | Loss: 0.44182 | Acc: 78.146\n",
      "Epoch 043: | Loss: 0.44196 | Acc: 78.132\n",
      "Epoch 044: | Loss: 0.44173 | Acc: 78.156\n",
      "Epoch 045: | Loss: 0.44177 | Acc: 78.148\n",
      "Epoch 046: | Loss: 0.44161 | Acc: 78.155\n",
      "Epoch 047: | Loss: 0.44160 | Acc: 78.157\n",
      "Epoch 048: | Loss: 0.44152 | Acc: 78.141\n",
      "Epoch 049: | Loss: 0.44176 | Acc: 78.153\n",
      "Epoch 050: | Loss: 0.44151 | Acc: 78.153\n"
     ]
    }
   ],
   "source": [
    "model = binaryClassification()\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss() # loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "\n",
    "    X_train, X_test = X.iloc[train_index,], X.iloc[test_index,]\n",
    "    y_train, y_test = y.iloc[train_index,], y.iloc[test_index,]\n",
    "\n",
    "    # standardize train_test_oot data\n",
    "    scaler = StandardScaler() \n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    oot_X = scaler.transform(oot_X)\n",
    "    \n",
    "    # oversampling\n",
    "    os = SMOTE()\n",
    "    columns = X.columns\n",
    "    os_data_X,os_data_y=os.fit_resample(X_train, y_train)\n",
    "    os_data_X = pd.DataFrame(data=os_data_X,columns=columns)\n",
    "    os_data_y= pd.DataFrame(data=os_data_y,columns=['fraud_label'])\n",
    "    os_data_y = os_data_y['fraud_label']\n",
    "\n",
    "    # set up train_test data and load dataset into pytorch\n",
    "    train_data = trainData(torch.FloatTensor(os_data_X.to_numpy()), \\\n",
    "                           torch.FloatTensor(os_data_y.to_numpy()))\n",
    "    \n",
    "    #train_data = trainData(torch.FloatTensor(X.to_numpy()), \\\n",
    "                      #     torch.FloatTensor(y.to_numpy()))\n",
    "\n",
    "    test_data = testData(torch.FloatTensor(X_test))\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "    #oot_loader = DataLoader(dataset=oot_X, batch_size=1)\n",
    "\n",
    "    model.train()\n",
    "    for e in range(1, EPOCHS+1):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(X_batch)\n",
    "\n",
    "            loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "            acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "\n",
    "        print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "divine-circumstances",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T18:04:02.197770Z",
     "start_time": "2021-03-21T18:03:45.398188Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_test_pred_list = []\n",
    "y_test_pred_prob_list = []\n",
    "model.eval()\n",
    "\n",
    "scaler = StandardScaler() \n",
    "X_test = scaler.fit_transform(X_test)\n",
    "test_data = testData(torch.FloatTensor(X_test))\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_test_pred_prob_list.append(y_test_pred)\n",
    "        y_test_pred_tag = torch.round(y_test_pred)\n",
    "        y_test_pred_list.append(y_test_pred_tag.cpu().numpy())\n",
    "\n",
    "\n",
    "y_test_pred_list = [a.squeeze().tolist() for a in y_test_pred_list]\n",
    "y_test_pred_prob_list = [a.squeeze().tolist() for a in y_test_pred_prob_list]\n",
    "\n",
    "result = pd.DataFrame(y_test_pred_prob_list,columns=['prob_1'])\n",
    "result.loc[:,'fraud_label'] = y_test.values.reshape(-1,)\n",
    "result.loc[:,'pred_class'] = pd.Series(y_test_pred_list).values.reshape(-1,)\n",
    "temp = result.sort_values('prob_1', ascending=False)\n",
    "pop = int(round(temp.shape[0]*0.03))\n",
    "temp1 = temp.head(pop)\n",
    "fdr = temp1.fraud_label.sum() / y_test.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "going-million",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T18:04:04.261231Z",
     "start_time": "2021-03-21T18:04:04.253859Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5922656578394283"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "incorporated-runner",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T18:05:08.235599Z",
     "start_time": "2021-03-21T18:04:50.481989Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_oot_pred_list = []\n",
    "y_oot_pred_prob_list = []\n",
    "model.eval()\n",
    "\n",
    "oot_X = oot.iloc[:,:-1]\n",
    "oot_y = oot.iloc[:,-1]\n",
    "scaler = StandardScaler() \n",
    "oot_X = scaler.fit_transform(oot_X)\n",
    "oot_X = testData(torch.FloatTensor(oot_X))\n",
    "oot_loader = DataLoader(dataset=oot_X, batch_size=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch in oot_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_oot_pred = model(X_batch)\n",
    "        y_oot_pred = torch.sigmoid(y_oot_pred)\n",
    "        y_oot_pred_prob_list.append(y_oot_pred)\n",
    "        y_oot_pred_tag = torch.round(y_oot_pred)\n",
    "        y_oot_pred_list.append(y_oot_pred_tag.cpu().numpy())\n",
    "\n",
    "\n",
    "y_oot_pred_list = [a.squeeze().tolist() for a in y_oot_pred_list]\n",
    "y_oot_pred_prob_list = [a.squeeze().tolist() for a in y_oot_pred_prob_list]\n",
    "\n",
    "result1 = pd.DataFrame(y_oot_pred_prob_list,columns=['prob_1'])\n",
    "result1.loc[:,'fraud_label'] = oot_y.values.reshape(-1,1)\n",
    "result1.loc[:,'pred_class'] = pd.Series(y_oot_pred_list).values.reshape(-1,1)\n",
    "temp11 = result1.sort_values('prob_1', ascending=False)\n",
    "pop = int(round(temp11.shape[0]*0.03))\n",
    "temp12 = temp11.head(pop)\n",
    "fdr_oot = temp12.fraud_label.sum() / oot_y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "promotional-tours",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T18:05:11.272347Z",
     "start_time": "2021-03-21T18:05:11.267610Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5452640402347024"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdr_oot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "offshore-assist",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T01:16:25.865123Z",
     "start_time": "2021-03-21T01:16:25.862561Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "715497    0\n",
       "715498    0\n",
       "715499    0\n",
       "715500    0\n",
       "715501    0\n",
       "         ..\n",
       "794991    0\n",
       "794992    0\n",
       "794993    0\n",
       "794994    0\n",
       "794995    0\n",
       "Name: fraud_label, Length: 79499, dtype: int64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "spanish-width",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-21T01:16:34.941769Z",
     "start_time": "2021-03-21T01:16:34.937012Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2779177162048699"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "naked-ownership",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-20T23:19:45.259530Z",
     "start_time": "2021-03-20T23:19:45.177791Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found Double",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-4eeb431f3a2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mX_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moot_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0my_oot_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0my_oot_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_oot_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0my_oot_pred_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_oot_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-810179b6e645>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Float but found Double"
     ]
    }
   ],
   "source": [
    "# y_oot_pred_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in oot_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_oot_pred = model(X_batch)\n",
    "        y_oot_pred = torch.sigmoid(y_oot_pred)\n",
    "        y_oot_pred_tag = torch.round(y_oot_pred)\n",
    "        y_oot_pred_list.append(y_oot_pred_tag.cpu().numpy())\n",
    "\n",
    "y_oot_pred_list = [a.squeeze().tolist() for a in y_oot_pred_list]\n",
    "\n",
    "result = pd.DataFrame([y_oot_pred_list, oot_y.tolist()],columns=['prob_1','fraud_label'])\n",
    "temp = result.sort_values('prob_1', ascending=False)\n",
    "pop = int(round(temp.shape[0]*0.03))\n",
    "temp1 = temp.head(pop)\n",
    "fdr = temp1.fraud_label.sum() / oot_y.tolist().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-anime",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-screen",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# define fdr in pytorch\n",
    "\n",
    "def fdr_cal(x_data, y_data, model_choice):\n",
    "    model = model_choice\n",
    "    pop = int(round(len(x_data)*0.03))\n",
    "    result = pd.DataFrame(model.predict_proba(x_data),columns=['prob_0', 'prob_1'])\n",
    "    temp = x_data.copy()\n",
    "    temp['fraud_label'] = y_data\n",
    "    temp['prob_1']= list(result.prob_1)\n",
    "    temp0 = temp.sort_values('prob_1', ascending=False)\n",
    "    temp1 = temp0.head(pop)\n",
    "    fdr = temp1.fraud_label.sum() / y_data.sum()\n",
    "    \n",
    "    return fdr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
